
input {
    kafka {
            id => "input_logstash_nginx1"
            bootstrap_servers => "10.66.204.156:9092,10.80.87.50:9092"
            auto_offset_reset => "latest"
#    max_partition_fetch_bytes => "10485760"
            decorate_events => true
            codec => "plain"
            consumer_threads => 6
            enable_auto_commit => "true"
            enable_metric => "true"
            exclude_internal_topics => "true"
            poll_timeout_ms => "1000"
            client_id => "nginx-pro"
            group_id => "logstash_nginx"
            topics_pattern => "log-nginx-pro"
            type => "rpc"
    }

    kafka {
            id => "input_logstash_nginx2"
            bootstrap_servers => "10.66.204.156:9092,10.80.87.50:9092"
            auto_offset_reset => "latest"
#    max_partition_fetch_bytes => "10485760"
            decorate_events => true
            codec => "plain"
            consumer_threads => 6
            enable_auto_commit => "true"
            enable_metric => "true"
            exclude_internal_topics => "true"
            poll_timeout_ms => "1000"
            client_id => "nginx-pro-web"
            group_id => "logstash_nginx_web"
            topics_pattern => "log-nginx-web-pro"
            type => "web"
    }
}


filter {
  json {
    source => "message"
    target => "message"
  }
  if [type] =='rpc' {
    ruby {

    init => "
    @mymapper = ['time_local','hostname','remote_addr','upstream_addr','request_time','upstream_response_time','upstream_connect_time','status','upstream_status','bytes_sent','remote_user','request_uri','http_user_agent','http_referer','scheme','request_method','http_x_forwarded_for']
    "

   code => "
    begin
      body = event.get('message')
      ctype = event.get('ctype')
      message = body['message']
      event.remove('@timestamp')
      event.remove('@version')
      event.remove('message')
      event.remove('type')
      buffer_map = LogStash::Event.new(Hash[@mymapper.zip(message.split('|'))])
      event.append(buffer_map)

      upstream_response_time  = event.get('upstream_response_time')
      if upstream_response_time == '-'
         event.set('upstream_response_time','0')
      end

      upstream_status  = event.get('upstream_status')
      if upstream_status == '-'
         event.set('upstream_status','0')
      end

      upstream_connect_time  = event.get('upstream_connect_time')
      if upstream_connect_time == '-'
         event.set('upstream_connect_time','0')
      end

      time_local = event.get('time_local')

      #datetime = DateTime.strptime(time_local,'%d/%b/%Y:%H:%M:%S %z')

      datetime = DateTime.strptime(time_local,'%Y-%m-%dT%H:%M:%S%z')
      source_timestamp = datetime.to_time.to_i * 1000
      source_date = datetime.strftime('%Y-%m-%d')
      event.set('source_timestamp',source_timestamp)
      event.set('source_date',source_date)
      event.set('domain','log')

      event.set('project','nginx-internal')

    rescue
      event.set('_drop','true')
    end
    "
    }
  }else {
      ruby {
    init => "
    @mymapper = ['time_local','hostname','remote_addr','upstream_addr','request_time','upstream_response_time','upstream_connect_time','status','upstream_status','bytes_sent','remote_user','uri','query_string','http_user_agent','http_referer','scheme','request_method','http_x_forwarded_for']
    "

  code => "
    begin
      body = event.get('message')
      ctype = event.get('ctype')
      message = body['message']
      event.remove('@timestamp')
      event.remove('@version')
      event.remove('message')
      event.remove('type')

      reqhash = Hash[@mymapper.zip(message.split('|'))]
      query_string = reqhash['query_string']    
      reqhash.delete('query_string')     
      query_param = CGI.parse(query_string)
      query_param.each { |key,value| query_param[key]=value.join() }
      reqhash['query_param'] = query_param
      buffer_map = LogStash::Event.new(reqhash)
      event.append(buffer_map)

      upstream_response_time  = event.get('upstream_response_time')
      if upstream_response_time == '-'
         event.set('upstream_response_time','0')
      end

      upstream_status  = event.get('upstream_status')
      if upstream_status == '-'
         event.set('upstream_status','0')
      end

      upstream_connect_time  = event.get('upstream_connect_time')
      if upstream_connect_time == '-'
         event.set('upstream_connect_time','0')
      end

      time_local = event.get('time_local')

      #datetime = DateTime.strptime(time_local,'%d/%b/%Y:%H:%M:%S %z')

      datetime = DateTime.strptime(time_local,'%Y-%m-%dT%H:%M:%S%z')
      source_timestamp = datetime.to_time.to_i * 1000
      source_date = datetime.strftime('%Y-%m-%d')
      event.set('source_timestamp',source_timestamp)
      event.set('source_date',source_date)
      event.set('domain','log')

      event.set('project','nginx-web')

    rescue
      event.set('_drop','true')
    end
    "

    }
      
  }
 

    if [_drop] == 'true' {
     drop{}
    }
}

output{

# stdout { codec => rubydebug }


  elasticsearch {
    id => "output_logstash_nginx2"
    action => "index"
    codec => "plain"
    doc_as_upsert => "false"
#    flush_size => 20000
    hosts => ["10.31.24.195:9200","10.81.33.180:9200"]
    index => "%{domain}-%{project}-%{source_date}"
    document_type => "%{project}"
    pool_max => 2000
    retry_on_conflict => 3
    sniffing => true
#   user => elastic
#   password => xxx
  }

}

